call MPI_Scatter(f_send, np, MPI_DOUBLE_PRECISION, f, np, MPI_DOUBLE_PRECISION, 0, MPI_COMM_WORLD, ierr)!offload f to GPUs!$acc enter data copyin(f)!Device pointer f will be passed to MPI_send & MPI_recv!$acc host_data use_device(f)if(myid.lt.nproc-1) then    call MPI_Send(f(np:np), 1, MPI_DOUBLE_PRECISION, myid+1, tag, MPI_COMM_WORLD, ierr)endifif(myid.gt.0) then    call MPI_Recv(f(1), 1, MPI_DOUBLE_PRECISION, myid-1, tag, MPI_COMM_WORLD, status, ierr)endif!$acc end host_data!do something .e.g.!$acc kernelsf = f/2.!$acc end kernelsSumToT=0d0!$acc parallel loop reduction(+:SumToT)do k=1,np    SumToT = SumToT + f(k)enddo!$acc end parallel loop!SumToT is by default copied back to the CPU!$acc data copy(SumToT)!$acc host_data use_device(SumToT)call MPI_ALLREDUCE(MPI_IN_PLACE, SumToT, 1, MPI_DOUBLE_PRECISION, MPI_SUM, MPI_COMM_WORLD, ierr)!$acc end host_data!$acc end data!$acc exit data delete(f)