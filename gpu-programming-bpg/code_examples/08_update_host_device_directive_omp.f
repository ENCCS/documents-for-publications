!! MPI-OpenMP!call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f, np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD, ierr)!offload f to GPUs!$omp target enter data device(myDevice) map(to:f)!update f: copy f from GPU to CPU!$omp target update device(myDevice) from(f)if(myid.lt.nproc-1) then    call MPI_Send(f(np:np), 1, MPI_DOUBLE_PRECISION, myid+1, tag, MPI_COMM_WORLD, ierr)endifif(myid.gt.0) then    call MPI_Recv(f(1), 1, MPI_DOUBLE_PRECISION, myid-1, tag, MPI_COMM_WORLD, status, ierr)endif!update f: copy f from CPU to GPU!$omp target update device(myDevice) to(f)!do something .e.g.!$omp target teams distribute parallel dodo k=1,np    f(k) = f(k) / 2.enddo!$omp end target teams distribute parallel doSumToT=0d0!$omp target teams distribute parallel do reduction(+:SumToT)do k=1,np    SumToT = SumToT + f(k)enddo!$omp end target teams distribute parallel do  !SumToT is by default copied back to the CPUcall MPI_ALLREDUCE(MPI_IN_PLACE, SumToT, 1, MPI_DOUBLE_PRECISION, MPI_SUM, MPI_COMM_WORLD, ierr )!$omp target exit data map(delete:f)