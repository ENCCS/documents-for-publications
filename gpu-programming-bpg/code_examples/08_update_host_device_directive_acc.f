!! MPI-OpenACC!call MPI_Scatter(f_send, np, MPI_DOUBLE_PRECISION, f, np, MPI_DOUBLE_PRECISION, 0, MPI_COMM_WORLD, ierr)!offload f to GPUs!$acc enter data copyin(f)!update f: copy f from GPU to CPU!$acc update host(f)if(myid.lt.nproc-1) then    call MPI_Send(f(np:np), 1, MPI_DOUBLE_PRECISION, myid+1, tag, MPI_COMM_WORLD, ierr)endifif(myid.gt.0) then    call MPI_Recv(f(1), 1, MPI_DOUBLE_PRECISION, myid-1, tag, MPI_COMM_WORLD, status, ierr)endif!update f: copy f from CPU to GPU!$acc update device(f)!do something .e.g.!$acc kernelsf = f/2.!$acc end kernelsSumToT=0d0!$acc parallel loop reduction(+:SumToT)do k=1,np    SumToT = SumToT + f(k)enddo!$acc end parallel loop!SumToT is by default copied back to the CPUcall MPI_ALLREDUCE(MPI_IN_PLACE, SumToT, 1, MPI_DOUBLE_PRECISION, MPI_SUM, MPI_COMM_WORLD, ierr)!$acc exit data delete(f)