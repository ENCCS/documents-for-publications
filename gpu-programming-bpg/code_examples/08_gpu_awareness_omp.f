call MPI_Scatter(f_send, np, MPI_DOUBLE_PRECISION, f, np, MPI_DOUBLE_PRECISION, 0, MPI_COMM_WORLD, ierr)!offload f to GPUs!$omp target enter data device(myDevice) map(to:f)!Device pointer f will be passed to MPI_send & MPI_recv!$omp target data use_device_ptr(f)if(myid.lt.nproc-1) then    call MPI_Send(f(np:np), 1, MPI_DOUBLE_PRECISION, myid+1, tag, MPI_COMM_WORLD, ierr)endifif(myid.gt.0) then    call MPI_Recv(f(1), 1, MPI_DOUBLE_PRECISION, myid-1, tag, MPI_COMM_WORLD, status, ierr)endif!$omp end target data!do something .e.g.!$omp target teams distribute parallel dodo k=1,np    f(k) = f(k)/2.enddo!$omp end target teams distribute parallel doSumToT=0d0!$omp target teams distribute parallel do reduction(+:SumToT)do k=1,np    SumToT = SumToT + f(k)enddo!$omp end target teams distribute parallel do  !SumToT is by default copied back to the CPU!$omp target enter data device(myDevice) map(to:SumToT)!$omp target data use_device_ptr(SumToT)call MPI_ALLREDUCE(MPI_IN_PLACE, SumToT, 1, MPI_DOUBLE_PRECISION, MPI_SUM, MPI_COMM_WORLD, ierr)!$omp end target data!$omp target exit data map(from:SumToT)!$omp target exit data map(delete:f)