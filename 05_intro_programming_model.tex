\section{Introduction to GPU Programming Models}


\par
There are different ways to use GPUs for computations.
In the best case, when someone has already written the code, one only needs to set the parameters and initial configurations in order to get started.
Or in some cases the problem is in such a way that it is only needed to use a library to solve the most intensive part of the code.
However these are quite limited cases and in general some programming might be needed.
There are several GPU programming software environments and APIs available, such as~\textbf{directive-based models},~\textbf{non-portable kernel-based models}, and~\textbf{portable kernel-based models}, as well as~\textbf{high-level frameworks and libraries}.
Below provides a general explanation of these contents, and the detailed description of these contents will be illustrated in the following sections.


\subsection{Standard C++/Fortran}


\par
Programs written in standard C++ and Fortran languages can now take advantage of NVIDIA GPUs without depending of any external library.
This is possible thanks to the NVIDIA Software Development Kit (SDK), which provides a suite of compilers~\cite{NVIDIA_SDK} that translates and optimizes the code for running on GPUs.
Guidelines for writing C++ and Fortran code are available at the community for developers using NVIDIA GPUs~\cite{guideline_gpu_cpp, guideline_gpu_fortran}.
The performance of these two approaches is promising as it can be seen in the examples provided in those guidelines.


% -------------------------------------------------------------------- %


\subsection{Directive-based programming}


\par
A fast and cheap way is to use directive based approaches.
In this case the existing serial code is annotated with hints which indicate to the compiler which loops and regions should be executed on the GPU.
In the absence of the API the directives are treated as comments and the code will just be executed as a usual serial code.
This approach is focused on productivity and easy usage (but to the detriment of performance), and allows employing accelerators with minimum programming effort by adding parallelism to existing code without the need to write accelerator-specific code.
There are two common ways to program using directives, namely~\textbf{OpenACC} and~\textbf{OpenMP}.


\subsubsection{OpenACC}

\par
OpenACC (Open Accelerator)~\cite{openacc} is developed by a consortium formed in 2010 with the goal of developing a standard, portable, and scalable programming model for accelerators, including GPUs.
Members of the OpenACC consortium include GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers, universities, and software companies.
Until recently it was supporting only NVIDIA GPUs, but now there is effort to support more and more devices and architectures.


\par
OpenACC provides directives, compiler directives, and APIs that enable developers to annotate existing code and offload computationally intensive portions to accelerators while maintaining portability across different platforms.
OpenACC relies on compiler directives to express parallelism and data management.
Directives are pragmas or comments inserted into the source code to guide the compiler in parallelizing and optimizing the code.
These directives indicate which parts of the code should be offloaded to an accelerator, such as a GPU, and how to distribute the workload for loop parallelization, data mapping, data privatization, and loop collapsing, among others.


\subsubsection{OpenMP}


\par
OpenMP (Open Multi-Processing)~\cite{openmp} is a multi-platform and shared-memory parallel programming API.
It provides a scalable approach to parallel computing on multi-core CPUs, and adds relatively recently support for GPU offloading.
It aims to support various types of GPUs.


\par
OpenMP also uses compiler directives, typically in the form of pragmas, to express parallelism and guide the compiler in parallelizing the code.
Directives are inserted into the source code to indicate which sections or loops should be executed in parallel.
These directives enable developers to parallelize their code by adding directives and library calls, enabling the execution of multiple threads to work on different parts of the code simultaneously.


\par
OpenMP is widely supported across different compilers, platforms, and programming languages, including C/C++ and Fortran.
In these three programming languages, you can control the level of parallelism, specify loop scheduling, handle reductions, and use other OpenMP constructs to optimize and control parallel execution.


% -------------------------------------------------------------------- %


\subsection{Non-portable kernel-based models}


\par
When doing direct GPU programming the developer has a large level of control by writing low-level code that directly communicates with the GPU and its hardware.
Theoretically direct GPU programming methods provide the ability to write low-level, GPU-accelerated code that can provide significant performance improvements over CPU-only code.
However, they also require a deeper understanding of the GPU architecture and its capabilities, as well as the specific programming method being used.
Multiple examples of CUDA/HIP code are available in the~\textbf{\textcolor{brown}{content/examples/cuda-hip}} directory of this repository~\cite{gpu-programming-examples}.


\par
Non-portable kernel-based models refer to native computational models or algorithms that are specifically designed and implemented for a particular hardware architecture or platform.
These models heavily rely on low-level optimizations and hardware-specific features, making them non-portable across different architectures or systems.
The advantage of non-portable kernel-based models is the potential for achieving highly optimized and efficient computations on specific hardware architectures.
These models can exploit fine-grained parallelism, utilize hardware-specific features, and leverage low-level optimizations.
Representatives are CUDA (for NVIDIA GPUs) and HIP (for AMD GPUs).


\subsubsection{CUDA}

\par
CUDA~\cite{cuda} is a parallel computing platform and API developed by NVIDIA.
It is historically the first mainstream GPU programming framework.
It allows developers to write C++-like code that is executed on the GPU.
CUDA provides a set of libraries, compilers, and development tools for low-level GPU programming and provides a performance boost for demanding computationally-intensive applications.
While there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.


\subsubsection{HIP}


\par
HIP~\cite{hip} is a C++ programming language and runtime API developed by AMD that provides a low-level interface for GPU programming.
HIP is designed to provide a single source code that can be used on both NVIDIA and AMD GPUs.
It is based on the CUDA programming model and provides an almost identical programming interface to CUDA.


% -------------------------------------------------------------------- %


\subsection{Portable kernel-based models}

\par
Cross-platform portability ecosystems typically provide a higher-level abstraction layer which provide a convenient and portable kernel-based programming models for GPU programming.
They can help reduce the time and effort required to maintain and deploy GPU-accelerated applications.
The goal of these ecosystems is achieving performance portability with a single-source application.
In C++, the most notable cross-platform portability ecosystems are~\textbf{Alpaka}~\cite{alpaka},~\textbf{Kokkos}~\cite{kokkos},~\textbf{OpenCL} (C and C++ APIs)~\cite{OpenCL},~\textbf{RAJA}~\cite{raja}, and~\textbf{SYCL}~\cite{sycl}.


\subsubsection{Kokkos}


\par
Kokkos~\cite{kokkos} is an open-source performance portable programming model for heterogeneous parallel computing that has been so far mostly developed at Sandia National Laboratories.
It is a C++-based ecosystem that provides a programming model for developing efficient and scalable parallel applications that run on many-core architectures such as CPUs, GPUs, and FPGAs.
The Kokkos ecosystem consists of several components, such as the Kokkos core library, which provides parallel execution and memory abstraction, the Kokkos kernels library, which provides math kernels for linear algebra and graph algorithms, and the Kokkos tools library, which provides profiling and debugging tools.
Kokkos components integrate well with other software libraries and technologies, such as MPI and OpenMP. 
Furthermore, the project collaborates with other projects, in order to provide interoperability and standardization for portable C++ programming.


\subsubsection{OpenCL}


\par
OpenCL (Open Computing Language)~\cite{OpenCL} is a cross-platform, open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs.
It supports a wide range of hardware from multiple vendors.
OpenCL provides a low-level programming interface for GPU programming and enables developers to write programs that can be executed on a variety of platforms.
Unlike programming models such as CUDA, HIP, Kokkos and SYCL, the OpenCL uses a separate-source model.
Recent versions of the OpenCL standard added C++ support for both API and the kernel code, but the C-based interface is still more widely used.
The OpenCL Working Group doesnâ€™t provide any frameworks of its own. Instead, vendors who produce OpenCL-compliant devices release frameworks as part of their SDKs.
The two most popular OpenCL SDKs are released by NVIDIA and AMD.
In both cases, the development kits are free and contain the libraries and tools that make it possible to build OpenCL applications.


\subsubsection{SYCL}


\par
SYCL~\cite{sycl} is a royalty-free, open-standard C++ programming model for multi-device programming.
It provides a high-level, single-source programming model for heterogeneous systems, including GPUs. 
Originally SYCL was developed on top of OpenCL, however it is not limited to just that.
It can be implemented on top of other low-level heterogeneous computing APIs, such as CUDA or HIP, enabling developers to write programs that can be executed on a variety of platforms.
Note that while SYCL is relatively high-level model, the developers are still required to write GPU kernels explicitly.


\par
While Alpaka~\cite{alpaka}, Kokkos~\cite{kokkos}, and RAJA~\cite{raja} refer to specific projects, SYCL itself is only a standard, for which several implementations exist.
For GPU programming, Intel oneAPI DPC++~\cite{oneapi-dpc} (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with Codeplay oneAPI plugins~\cite{codeplay-oneapi}) and hipSYCL~\cite{hipsycl} (also known as Open SYCL, supporting NVIDIA and AMD GPUs, with experimental Intel GPU support available in combination with Intel oneAPI DPC++) are the most widely used. 
Other implementations of note are triSYCL~\cite{trisycl} and ComputeCPP~\cite{computecpp}.


% -------------------------------------------------------------------- %


\subsection{High-level language-based programming}


\par
GPU programming traditionally relied on low-level languages like CUDA and OpenCL, which required explicit management of memory, thread synchronization, and other low-level details.
As GPUs became more widely adopted for general-purpose computing, high-level language support for GPU programming has also emerged.
Below are representative high-level languages that provide support for GPU programming.


\subsubsection{Python}

\par
Python offers support for GPU programming through several libraries, including:
\begin{itemize}
    \item~\textbf{CuPy}: CuPy~\cite{cupy} is a GPU-based data array library compatible with~\textbf{NumPy}/\textbf{SciPy}. It offers a highly similar interface to NumPy and SciPy, making it easy for developers to transition to GPU computing. To utilize CuPy, simply replace $numpy$ and $scipy$ with $cupy$ and $cupyx.scipy$ in your Python code.
    \item~\textbf{cuDF}: RAPIDS~\cite{rapids} is a high level package collections which implement CUDA functionalities and API with Python bindings. cuDF~\cite{cudf} belongs to RAPIDS and is the library for manipulating data frames on GPU. cuDF provides a pandas-like API, so if you are familiar with Pandas~\cite{pandas}, you can accelerate your work without knowing too much CUDA programming.
    \item~\textbf{PyCUDA}: PyCUDA~\cite{pycuda} is a Python programming environment for CUDA. It allows users to access to NVIDIAâ€™s CUDA API from Python. PyCUDA is powerful library but only runs on NVIDIA GPUs. Knowledge of CUDA programming is needed.
    \item~\textbf{Numba}: Similarly as for CPUs, Numba~\cite{numba} allows users to just-in-time~\cite{jit} (JIT) compile Python code to work on GPU as well. Numba supports GPUs from NVIDIA and will likely support AMD GPUs in the future.
    \item~\textbf{PyTorch}: PyTorch~\cite{pytorch} is a deep learning framework that allows seamless integration with GPUs. It provides tensor operations and automatic differentiation capabilities, enabling efficient GPU-accelerated neural network training and inference.
    \item~\textbf{TensorFlow}: TensorFlow~\cite{tensorflow} is another widely used deep learning framework that supports GPU acceleration. It provides a high-level API for defining and training neural networks and takes advantage of GPUs to accelerate computations.
\end{itemize}


\subsubsection{Julia}


\par
Julia~\cite{julia} is a high-level, dynamic programming language designed for numerical and scientific computing.
It offers built-in support for parallel and distributed computing, including GPU acceleration.
Julia has first-class support for GPU programming through the following packages that target GPUs from all three major vendors:
\begin{itemize}
    \item~\textbf{\textcolor{brown}{CUDA.jl}} for NVIDIA GPUs~\cite{cudajl}
    \item~\textbf{\textcolor{brown}{AMDGPU.jl}} for AMD GPUs~\cite{amdgpujl}
    \item~\textbf{\textcolor{brown}{oneAPI.jl}} for Intel GPUs~\cite{oneapijl}
    \item~\textbf{\textcolor{brown}{Metal.jl}} for Apple M-series GPUs~\cite{metaljl}
\end{itemize}

\par
\textbf{\textcolor{brown}{CUDA.jl}} is the most mature,~\textbf{\textcolor{brown}{AMDGPU.jl}} is somewhat behind but still ready for general use, while~\textbf{\textcolor{brown}{oneAPI.jl}} and~\textbf{\textcolor{brown}{Metal.jl}} are functional but might contain bugs, miss some features and provide suboptimal performance.
Their respective APIs are however completely analogous and translation between libraries is straightforward.
All packages offer both high-level abstractions that require very little programming effort and a lower level approach for writing kernels for fine-grained control.


\subsubsection{Other high-level programming language}


Besides Python and Julia, there are other high-level language support GPU programming, such as~\textbf{MATLAB} and~\textbf{R}.
\begin{itemize}
    \item MATLAB: MATLAB~\cite{matlab} is a popular programming environment for numerical computing. It provides support for GPU computing through the Parallel Computing Toolbox. MATLAB allows users to perform computations on GPUs by utilizing GPU-enabled functions and leveraging the CUDA programming language under the hood.
    \item R: R~\cite{rproject} is a language widely used in statistical computing and data analysis. GPU acceleration is available for certain operations in R through packages such as~\textbf{gputools} and~\textbf{gpuR}. These packages provide functions that allow users to offload computations to GPUs and leverage parallel processing capabilities.
\end{itemize}


\par
It's important to note that while these high-level languages offer GPU programming support, they still rely on lower-level GPU programming frameworks or libraries underneath (such as CUDA or OpenCL) to communicate with the GPU and perform the actual parallel computations.
However, the high-level language interfaces simplify the development process and provide a more user-friendly experience for GPU programming.


% -------------------------------------------------------------------- %


\subsection{Summary}


\par
Each of these GPU programming environments has its own strengths and weaknesses, and the best choice for a given project will depend on a range of factors, including:
\begin{itemize}
    \item the hardware platforms being targeted
    \item the type of computation being performed, and
    \item the developerâ€™s experience and preferences.
\end{itemize}


\par
\textbf{High-level and productivity-focused APIs} provide a simplified programming model and maximize code portability, while~\textbf{low-level and performance-focused APIs} provide a high level of control over the GPUâ€™s hardware but also require more coding effort and expertise.


\begin{itemize}
    \item \textbf{Directive-based programming}:
    \begin{itemize}
        \item Existing serial code is annotated with directives to indicate which parts should be executed on the GPU.
        \item OpenACC and OpenMP are common directive-based programming models.
        \item Productivity and easy usage are prioritized over performance.
        \item Minimum programming effort is required to add parallelism to existing code.
    \end{itemize}
    \item \textbf{Non-portable kernel-based models}:
    \begin{itemize}
        \item Low-level code is written to directly communicate with the GPU.
        \item CUDA is NVIDIAâ€™s parallel computing platform and API for GPU programming.
        \item HIP is an API developed by AMD that provides a similar programming interface to CUDA for both NVIDIA and AMD GPUs.
        \item Deeper understanding of GPU architecture and programming methods is needed.
    \end{itemize}
    \item \textbf{Portable kernel-based models}:
    \begin{itemize}
        \item Higher-level abstractions for GPU programming that provide portability.
        \item Examples include Alpaka, Kokkos, OpenCL, RAJA, and SYCL.
        \item Aim to achieve performance portability with a single-source application.
        \item Can run on various GPUs and platforms, reducing the effort required to maintain and deploy GPU-accelerated applications.
    \end{itemize}
    \item \textbf{High-level language-based programming}:
    \begin{itemize}
        \item Python libraries like CuPy, cuDF, PyCUDA, and Numba offer GPU programming capabilities.
        \item Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU programming.
        \item These libraries provide high-level abstractions and interfaces for GPU programming in their respective languages.
    \end{itemize}
\end{itemize}
